# @tuon-live-transcribe

Real-time, streaming speech-to-text service using FastAPI and OpenAI's gpt-4o-transcribe model.

## Project Purpose

This service provides a WebSocket endpoint for live audio transcription. It acts as a gateway between a client application (e.g., a web browser capturing microphone input) and the OpenAI Realtime Transcription API.

Key features:
-   Receives audio chunks over WebSocket from a client.
-   Connects to OpenAI's Realtime API using an ephemeral token.
-   Streams audio to OpenAI.
-   Receives partial and final transcripts from OpenAI.
-   Relays these transcripts back to the connected client in near real-time.

## Setup and Running Locally

1.  **Prerequisites:**
    *   Python 3.11+
    *   `pip` for installing dependencies.

2.  **Clone the repository (if applicable) or ensure you are in the `tuon-live-transcribe` project root.**

3.  **Create a Python virtual environment (recommended):**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

4.  **Install dependencies:**
    ```bash
    pip3 install -r requirements.txt
    ```

5.  **Configure Environment Variables:**
    Create a `.env` file in the `tuon-live-transcribe` project root with your OpenAI API key:
    ```env
    OPENAI_API_KEY="sk-your-openai-api-key-here"
    # Optional: You can also override these if needed (defaults are in config.py)
    # MODEL_NAME="gpt-4o-transcribe"
    # PORT=8000
    # LOG_LEVEL="INFO"
    ```
    Replace `sk-your-openai-api-key-here` with your actual OpenAI API key.

6.  **Run the application:**
    ```bash
    python3 -m tuon_live_transcribe.main
    ```
    The service will start, typically on `http://localhost:8000`.

7.  **Test with the included HTML page:**
    Open your browser and navigate to `http://localhost:8000/static/index.html`.
    You should be able to start recording, and see live transcripts appear.

## API Overview

*   **`GET /`**: Health check endpoint.
    *   Returns `{"status": "ok", "model_name": "gpt-4o-transcribe"}` (or the configured model).
*   **`WS /v1/transcriptions`**: WebSocket endpoint for live transcription.
    *   **Client Handshake:** Client sends a JSON message: `{"action": "start", "language": "en"}` (language is optional, defaults to "en").
    *   **Server Response:** Server replies with `{"session_id": "<unique-session-id>"}`.
    *   **Audio Streaming:** Client sends binary audio data (Opus-encoded WebM chunks recommended, server expects PCM16 as per current OpenAI config but client-side `main.js` uses WebM/Opus).
    *   **Transcript Streaming:** Server sends JSON messages:
        *   `{"partial": "text..."}` for partial transcripts.
        *   `{"final": "text..."}` for final transcripts.
        *   `{"error": "message..."}` if an error occurs.

## Building and Running with Docker

1.  **Build the Docker image:**
    From the `tuon-live-transcribe` project root:
    ```bash
    docker build -t tuon-live-transcribe .
    ```

2.  **Run the Docker container:**
    You need to pass your `OPENAI_API_KEY` to the container. The easiest way for local development is using the `.env` file:
    ```bash
    docker run -p 8000:8000 --env-file .env tuon-live-transcribe
    ```
    Alternatively, you can pass the environment variable directly:
    ```bash
    docker run -p 8000:8000 -e OPENAI_API_KEY="sk-your-openai-api-key-here" tuon-live-transcribe
    ```
    The service inside the container will be accessible on `http://localhost:8000`.

## Deployment Notes (from PRD)

*   Consider 512 MiB RAM, scale to 1 GiB for >10 concurrent sessions.
*   Adjust keep-alive timeouts on deployment platforms (e.g., Render ~100s) for long WebSocket sessions.
*   Future: Protect endpoint with signed JWTs generated by the main application.

## Audio Format Note

The backend is currently configured to tell OpenAI it's sending `pcm16`. The provided `static/main.js` uses `audio/webm; codecs=opus` with `MediaRecorder`. For a production system, ensure the client sends audio in a format compatible with what OpenAI's Realtime API expects or add transcoding/format negotiation in the gateway service (`tuon-live-transcribe`). The PRD mentions PCM or Opus-encoded WebM as client-side options. If Opus is sent, the `input_audio_format` in `connect_to_openai_realtime_api` in `main.py` should be updated accordingly (e.g. to `opus`).